{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `keras-unet-collection.models` user guide\n",
    "\n",
    "This user guide requires `keras-unet-collection==0.1.9` or higher.\n",
    "\n",
    "## Content\n",
    "\n",
    "* [**U-net**](#U-net)\n",
    "* [**V-net**](#V-net)\n",
    "* [**Attention-Unet**](#Attention-Unet)\n",
    "* [**U-net++**](#U-net++)\n",
    "* [**UNET 3+**](#UNET-3+)\n",
    "* [**R2U-net**](#R2U-net)\n",
    "* [**ResUnet-a**](#ResUnet-a)\n",
    "* [**U^2-Net**](#U^2-Net)\n",
    "* [**TransUNET**](#TransUNET)\n",
    "* [**Swin-UNET**](#Swin-UNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: importing `models` from `keras_unet_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: defining your hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used hyper-parameter options are listed as follows. Full details are available through the Python helper function:\n",
    "\n",
    "* `input_size`: a tuple or list that defines the shape of input tensors. \n",
    "    * `models.resunet_a_2d`, `models.transunet_2d`, and `models.swin_unet_2d` support int only, others also support `input_size=(None, None, 3)`.\n",
    "    * `activation='PReLU'` is not compatible with `input_size=(None, None, 3)`. \n",
    "\n",
    "\n",
    "* `filter_num`: a list that defines the number of convolutional filters per down- and up-sampling blocks.\n",
    "    * For `unet_2d`, `att_unet_2d`, `unet_plus_2d`, `r2_unet_2d`, depth $\\ge$ 2 is expected.\n",
    "    * For `resunet_a_2d` and `u2net_2d`, depth $\\ge$ 3 is expected.\n",
    "\n",
    "\n",
    "* `n_labels`: number of output targets, e.g., `n_labels=2` for binary classification.\n",
    "\n",
    "* `activation`: the activation function of hidden layers. Available choices are `'ReLU'`, `'LeakyReLU'`, `'PReLU'`, `'ELU'`, `'GELU'`, `'Snake'`.\n",
    "\n",
    "* `output_activation`: the activation function of the output layer. Recommended choices are `'Sigmoid'`, `'Softmax'`, `None` (linear), `'Snake'`.\n",
    "\n",
    "* `batch_norm`: if specified as True, all convolutional layers will be configured as stacks of \"Conv2D-BN-Activation\".\n",
    "\n",
    "* `stack_num_down`: number of convolutional layers per downsampling level.\n",
    "\n",
    "* `stack_num_up`: number of convolutional layers (after concatenation) per upsampling level. \n",
    "\n",
    "* `pool`: the configuration of downsampling (encoding) blocks.\n",
    "    * `pool=False`: downsampling with a convolutional layer (2-by-2 convolution kernels with 2 strides; optional batch normalization and activation). \n",
    "    * `pool=True` or `pool='max'` downsampling with a max-pooling layer.\n",
    "    * `pool='ave'` downsampling with a average-pooling layer.\n",
    "\n",
    "\n",
    "* `unpool`: the configuration of upsampling (decoding) blocks.\n",
    "    * `unpool=False`: upsampling with a transpose convolutional layer (2-by-2 convolution kernels with 2 strides; optional batch normalization and activation). \n",
    "    * `unpool=True` or `unpool='bilinear'` upsampling with bilinear interpolation.\n",
    "    * `unpool='nearest'` upsampling with reflective padding.\n",
    "\n",
    "\n",
    "* `name`: user-specified prefix of the configured layer and model. Use `keras.models.Model.summary` to identify the exact name of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Configuring your model\n",
    "\n",
    "**Note**\n",
    "\n",
    "Configured models can be saved through `model.save(filepath, save_traces=True)`, but they may contain python objects that are not part of the `tensorflow.keras`. Thus when loading the model, it is preferred to load the weights only, and set/freeze them within a new configuration.\n",
    "\n",
    "```python\n",
    "e.g.\n",
    "\n",
    "weights = dummy_loader(model_old_path)\n",
    "model_new = swin_transformer_model(...)\n",
    "model_new.set_weights(weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: U-net for binary classification with:\n",
    "\n",
    "1. Five down- and upsampliung levels (or four downsampling levels and one bottom level).\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. One convolutional layer (after concatenation) per upsamling level.\n",
    "\n",
    "2. Gaussian Error Linear Unit (GELU) activcation, Softmax output activation, batch normalization.\n",
    "\n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512, 1024],\n",
    "    n_labels=2,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"GELU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=\"nearest\",\n",
    "    name=\"unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: Vnet (originally proposed for 3-d inputs, here modified for 2-d inputs) for binary classification with:\n",
    "\n",
    "1. Input size of (256, 256, 1); PReLU does not support input tensor with shapes of NoneType \n",
    "\n",
    "\n",
    "\n",
    "1. Five down- and upsampliung levels (or four downsampling levels and one bottom level).\n",
    "\n",
    "1. Number of stacked convolutional layers of the residual path increase with downsampling levels from one to three (symmetrically, decrease with upsampling levels).   \n",
    "    * `res_num_ini=1`\n",
    "    * `res_num_max=3`\n",
    "    \n",
    " \n",
    "2. PReLU activcation, Softmax output activation, batch normalization.\n",
    "\n",
    "3. Downsampling through stride convolutional layers.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vnet_2d(\n",
    "    (256, 256, 1),\n",
    "    filter_num=[16, 32, 64, 128, 256],\n",
    "    n_labels=2,\n",
    "    res_num_ini=1,\n",
    "    res_num_max=3,\n",
    "    activation=\"PReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    name=\"vnet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**: attention-Unet for single target regression with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. ReLU activation, linear output activation (None), batch normalization.\n",
    "\n",
    "3. Additive attention, ReLU attention activation.\n",
    "        \n",
    "4. Downsampling through stride convolutional layers.\n",
    "\n",
    "5. Upsampling through bilinear interpolation.   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.att_unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=1,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    activation=\"ReLU\",\n",
    "    atten_activation=\"ReLU\",\n",
    "    attention=\"add\",\n",
    "    output_activation=None,\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"bilinear\",\n",
    "    name=\"attunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4**: U-net++ for three-label classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. LeakyReLU activation, Softmax output activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers.\n",
    "\n",
    "5. Deep supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\txnet_output_sup0_activation\n",
      "\txnet_output_sup1_activation\n",
      "\txnet_output_sup2_activation\n",
      "\txnet_output_final_activation\n"
     ]
    }
   ],
   "source": [
    "model = models.unet_plus_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=3,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    activation=\"LeakyReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=False,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"xnet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET 3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5**: UNet 3+ for binary classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. One convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers.\n",
    "\n",
    "5. Deep supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = [64, 64, 64]\n",
      "\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = 256\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet3plus_output_sup0_activation\n",
      "\tunet3plus_output_sup1_activation\n",
      "\tunet3plus_output_sup2_activation\n",
      "\tunet3plus_output_final_activation\n"
     ]
    }
   ],
   "source": [
    "model = models.unet_3plus_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_skip=\"auto\",\n",
    "    filter_num_aggregate=\"auto\",\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"unet3plus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `filter_num_skip` and `filter_num_aggregate` can be specified explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tunet3plus_output_sup0_activation\n",
      "\tunet3plus_output_sup1_activation\n",
      "\tunet3plus_output_sup2_activation\n",
      "\tunet3plus_output_final_activation\n"
     ]
    }
   ],
   "source": [
    "model = models.unet_3plus_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_skip=[64, 64, 64],\n",
    "    filter_num_aggregate=256,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"unet3plus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 6**: R2U-net for binary classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two recurrent convolutional layers with two iterations per down- and upsampling level.\n",
    "\n",
    "2. ReLU activation, Softmax output activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.r2_unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=2,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    recur_num=2,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=\"nearest\",\n",
    "    name=\"r2unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResUnet-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 7**: ResUnet-a for 16-label classification with:\n",
    "\n",
    "1. input size of (128, 128, 3)\n",
    "\n",
    "1. Six downsampling levels followed by an Atrous Spatial Pyramid Pooling (ASPP) layer with 256 filters.\n",
    "\n",
    "1. Six upsampling levels followed by an ASPP layer with 128 filters.\n",
    "\n",
    "2. dilation rates of {1, 3, 15, 31} for shallow layers, {1,3,15} for intermediate layers, and {1,} for deep layers.\n",
    "\n",
    "3. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "\n",
    "4. Downsampling through stride convolutional layers.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received dilation rates: [1, 3, 15, 31]\n",
      "Received dilation rates are not defined on a per downsampling level basis.\n",
      "Automated determinations are applied with the following details:\n",
      "\tdepth-0, dilation_rate = [1, 3, 15, 31]\n",
      "\tdepth-1, dilation_rate = [1, 3, 15, 31]\n",
      "\tdepth-2, dilation_rate = [1, 3, 15]\n",
      "\tdepth-3, dilation_rate = [1, 3, 15]\n",
      "\tdepth-4, dilation_rate = [1]\n",
      "\tdepth-5, dilation_rate = [1]\n"
     ]
    }
   ],
   "source": [
    "model = models.resunet_a_2d(\n",
    "    (128, 128, 3),\n",
    "    [32, 64, 128, 256, 512, 1024],\n",
    "    dilation_num=[1, 3, 15, 31],\n",
    "    n_labels=16,\n",
    "    aspp_num_down=256,\n",
    "    aspp_num_up=128,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"nearest\",\n",
    "    name=\"resunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dilation_num` can be specified per down- and uplampling level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resunet_a_2d(\n",
    "    (128, 128, 3),\n",
    "    [32, 64, 128, 256, 512, 1024],\n",
    "    dilation_num=[\n",
    "        [1, 3, 15, 31],\n",
    "        [1, 3, 15, 31],\n",
    "        [1, 3, 15],\n",
    "        [1, 3, 15],\n",
    "        [\n",
    "            1,\n",
    "        ],\n",
    "        [\n",
    "            1,\n",
    "        ],\n",
    "    ],\n",
    "    n_labels=16,\n",
    "    aspp_num_down=256,\n",
    "    aspp_num_up=128,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"nearest\",\n",
    "    name=\"resunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U^2-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 8**: U^2-Net for binary classification with:\n",
    "\n",
    "1. Six downsampling levels with the first four layers built with RSU, and the last two (one downsampling layer, one bottom layer) built with RSU-F4.\n",
    "    * `filter_num_down=[64, 128, 256, 512]`\n",
    "    * `filter_mid_num_down=[32, 32, 64, 128]`\n",
    "    * `filter_4f_num=[512, 512]`\n",
    "    * `filter_4f_mid_num=[256, 256]`\n",
    "    \n",
    "    \n",
    "1. Six upsampling levels with the deepest layer built with RSU-F4, and the other four layers built with RSU.\n",
    "    * `filter_num_up=[64, 64, 128, 256]`\n",
    "    * `filter_mid_num_up=[16, 32, 64, 128]`\n",
    "    \n",
    "    \n",
    "3. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "\n",
    "4. Deep supervision\n",
    "\n",
    "5. Downsampling through stride convolutional layers.\n",
    "\n",
    "6. Upsampling through transpose convolutional layers.\n",
    "\n",
    "*In the original work of U^2-Net, down- and upsampling were achieved through maxpooling (`pool=True` or `pool='max'`) and bilinear interpolation (`unpool=True` or unpool=`'bilinear'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 6\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tu2net_output_sup0_trans_conv\n",
      "\tu2net_output_sup1_trans_conv\n",
      "\tu2net_output_sup2_trans_conv\n",
      "\tu2net_output_sup3_trans_conv\n",
      "\tu2net_output_sup4_trans_conv\n",
      "\tu2net_output_sup5_trans_conv\n",
      "\tu2net_output_final\n"
     ]
    }
   ],
   "source": [
    "model = models.u2net_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_up=[64, 64, 128, 256],\n",
    "    filter_mid_num_down=[32, 32, 64, 128],\n",
    "    filter_mid_num_up=[16, 32, 64, 128],\n",
    "    filter_4f_num=[512, 512],\n",
    "    filter_4f_mid_num=[256, 256],\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=None,\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"u2net\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `u2net_2d` supports automated determination of filter numbers per down- and upsampling level. Auto-mode may produce a slightly larger network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [256, 256]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 6\n",
      "----------\n",
      "deep_supervision = True\n",
      "names of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n",
      "\"final\" is the final output layer):\n",
      "\n",
      "\tu2net_output_sup0_activation\n",
      "\tu2net_output_sup1_activation\n",
      "\tu2net_output_sup2_activation\n",
      "\tu2net_output_sup3_activation\n",
      "\tu2net_output_sup4_activation\n",
      "\tu2net_output_sup5_activation\n",
      "\tu2net_output_final_activation\n"
     ]
    }
   ],
   "source": [
    "model = models.u2net_2d(\n",
    "    (None, None, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"u2net\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransUNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 9**: TransUNET for 12-label classification with:\n",
    "\n",
    "* input size of (512, 512, 3)\n",
    "\n",
    "* Four down- and upsampling levels.\n",
    "\n",
    "* Two convolutional layers per downsampling level.\n",
    "\n",
    "* Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "* 12 transformer blocks (`num_transformer=12`).\n",
    "\n",
    "* 12 attention heads (`num_heads=12`).\n",
    "\n",
    "* 3072 MLP nodes per vision transformer (`num_mlp=3072`).\n",
    "\n",
    "* 768 embeding dimensions (`embed_dim=768`).\n",
    "\n",
    "* Gaussian Error Linear Unit (GELU) activcation for transformer MLPs.\n",
    "\n",
    "* ReLU activation, softmax output activation, batch normalization.\n",
    "\n",
    "* Downsampling through maxpooling.\n",
    "\n",
    "* Upsampling through bilinear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.transunet_2d(\n",
    "    (512, 512, 3),\n",
    "    filter_num=[64, 128, 256, 512],\n",
    "    n_labels=12,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    embed_dim=768,\n",
    "    num_mlp=3072,\n",
    "    num_heads=12,\n",
    "    num_transformer=12,\n",
    "    activation=\"ReLU\",\n",
    "    mlp_activation=\"GELU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=True,\n",
    "    unpool=\"bilinear\",\n",
    "    name=\"transunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 10**: Swin-UNET for 3-label classification with:\n",
    "\n",
    "* input size of (128, 128, 3)\n",
    "\n",
    "* Four down- and upsampling levels (or three downsampling levels and one bottom level) (`depth=4`).\n",
    "\n",
    "* Two Swin-Transformers per downsampling level.\n",
    "\n",
    "* Two Swin-Transformers (after concatenation) per upsampling level.\n",
    "\n",
    "* Extract 2-by-2 patches from the input (`patch_size=(2, 2)`)\n",
    "\n",
    "* Embed 2-by-2 patches to 64 dimensions (`filter_num_begin=64`, a.k.a, number of embedded dimensions).\n",
    "\n",
    "* Number of attention heads for each down- and upsampling level: `num_heads=[4, 8, 8, 8]`.\n",
    "\n",
    "* Size of attention windows for each down- and upsampling level: `window_size=[4, 2, 2, 2]`.\n",
    "\n",
    "* 512 nodes per Swin-Transformer (`num_mlp=512`)\n",
    "\n",
    "* Shift attention windows (i.e., Swin-MSA) (`shift_window=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling SwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'swin_transformer_block' (of type SwinTransformerBlock). Either the `SwinTransformerBlock.call()` method is incorrect, or you need to implement the `SwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nLayer.add_weight() got multiple values for argument 'shape'\u001b[0m\n\nArguments received by SwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 4096, 64), dtype=float32, sparse=False, name=keras_tensor_2660>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin_unet_2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_num_begin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_num_down\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_num_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSoftmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mswin_unet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_swin_unet_2d.py:310\u001b[0m, in \u001b[0;36mswin_unet_2d\u001b[0;34m(input_size, filter_num_begin, n_labels, depth, stack_num_down, stack_num_up, patch_size, num_heads, window_size, num_mlp, output_activation, shift_window, name)\u001b[0m\n\u001b[1;32m    307\u001b[0m IN \u001b[38;5;241m=\u001b[39m Input(input_size)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# base\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mswin_unet_2d_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_num_begin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_num_begin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_num_down\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_num_down\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_num_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_num_up\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# output layer\u001b[39;00m\n\u001b[1;32m    325\u001b[0m OUT \u001b[38;5;241m=\u001b[39m CONV_output(\n\u001b[1;32m    326\u001b[0m     X,\n\u001b[1;32m    327\u001b[0m     n_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name),\n\u001b[1;32m    331\u001b[0m )\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_swin_unet_2d.py:148\u001b[0m, in \u001b[0;36mswin_unet_2d_base\u001b[0;34m(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, patch_size, num_heads, window_size, num_mlp, shift_window, name)\u001b[0m\n\u001b[1;32m    145\u001b[0m X \u001b[38;5;241m=\u001b[39m patch_embedding(num_patch_x \u001b[38;5;241m*\u001b[39m num_patch_y, embed_dim)(X)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# The first Swin Transformer stack\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mswin_transformer_stack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstack_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstack_num_down\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_patch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_patch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patch_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_swin_down0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m X_skip\u001b[38;5;241m.\u001b[39mappend(X)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Downsampling blocks\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_swin_unet_2d.py:52\u001b[0m, in \u001b[0;36mswin_transformer_stack\u001b[0;34m(X, stack_num, embed_dim, num_patch, num_heads, window_size, num_mlp, shift_window, name)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m         shift_size_temp \u001b[38;5;241m=\u001b[39m shift_size\n\u001b[0;32m---> 52\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mSwinTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_patch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_patch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshift_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift_size_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/transformer_layers.py:719\u001b[0m, in \u001b[0;36mSwinTransformerBlock.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    714\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    715\u001b[0m     x_windows, newshape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n\u001b[1;32m    716\u001b[0m )\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Window-based multi-headed self-attention\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# Merge windows\u001b[39;00m\n\u001b[1;32m    722\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    723\u001b[0m     attn_windows, newshape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, C)\n\u001b[1;32m    724\u001b[0m )\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/transformer_layers.py:461\u001b[0m, in \u001b[0;36mWindowAttention.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# zero initialization\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     num_window_elements \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    460\u001b[0m     )\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_bias_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_attn_pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_window_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Indices of relative positions\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     coords_h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling SwinTransformerBlock.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'swin_transformer_block' (of type SwinTransformerBlock). Either the `SwinTransformerBlock.call()` method is incorrect, or you need to implement the `SwinTransformerBlock.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nLayer.add_weight() got multiple values for argument 'shape'\u001b[0m\n\nArguments received by SwinTransformerBlock.call():\n  • args=('<KerasTensor shape=(None, 4096, 64), dtype=float32, sparse=False, name=keras_tensor_2660>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "model = models.swin_unet_2d(\n",
    "    (128, 128, 3),\n",
    "    filter_num_begin=64,\n",
    "    n_labels=3,\n",
    "    depth=4,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    patch_size=(2, 2),\n",
    "    num_heads=[4, 8, 8, 8],\n",
    "    window_size=[4, 2, 2, 2],\n",
    "    num_mlp=512,\n",
    "    output_activation=\"Softmax\",\n",
    "    shift_window=True,\n",
    "    name=\"swin_unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-unet-collection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
