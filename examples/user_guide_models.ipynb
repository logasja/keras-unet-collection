{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `keras-unet-collection.models` user guide\n",
    "\n",
    "This user guide requires `keras-unet-collection==0.1.9` or higher.\n",
    "\n",
    "## Content\n",
    "\n",
    "* [**U-net**](#U-net)\n",
    "* [**V-net**](#V-net)\n",
    "* [**Attention-Unet**](#Attention-Unet)\n",
    "* [**U-net++**](#U-net++)\n",
    "* [**UNET 3+**](#UNET-3+)\n",
    "* [**R2U-net**](#R2U-net)\n",
    "* [**ResUnet-a**](#ResUnet-a)\n",
    "* [**U^2-Net**](#U^2-Net)\n",
    "* [**TransUNET**](#TransUNET)\n",
    "* [**Swin-UNET**](#Swin-UNET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: importing `models` from `keras_unet_collection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PreLU' from 'keras.layers' (/Users/meeko/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/keras/api/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/models.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_model_unet_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unet_2d\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_model_vnet_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vnet_2d\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_model_unet_plus_2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unet_plus_2d\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_unet_2d.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GELU, Snake\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_backbone_zoo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backbone_zoo, bach_norm_checker\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/layer_utils.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_unet_collection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GELU, Snake\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops, layers, activations\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreLU\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_layer\u001b[39m(X, channel, pool_size, unpool, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[1;32m      9\u001b[0m                  activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    An overall decode layer, based on either upsampling or trans conv.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreLU' from 'keras.layers' (/Users/meeko/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/keras/api/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras_unet_collection import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: defining your hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used hyper-parameter options are listed as follows. Full details are available through the Python helper function:\n",
    "\n",
    "* `input_size`: a tuple or list that defines the shape of input tensors. \n",
    "    * `models.resunet_a_2d`, `models.transunet_2d`, and `models.swin_unet_2d` support int only, others also support `input_size=(None, None, 3)`.\n",
    "    * `activation='PReLU'` is not compatible with `input_size=(None, None, 3)`. \n",
    "\n",
    "\n",
    "* `filter_num`: a list that defines the number of convolutional filters per down- and up-sampling blocks.\n",
    "    * For `unet_2d`, `att_unet_2d`, `unet_plus_2d`, `r2_unet_2d`, depth $\\ge$ 2 is expected.\n",
    "    * For `resunet_a_2d` and `u2net_2d`, depth $\\ge$ 3 is expected.\n",
    "\n",
    "\n",
    "* `n_labels`: number of output targets, e.g., `n_labels=2` for binary classification.\n",
    "\n",
    "* `activation`: the activation function of hidden layers. Available choices are `'ReLU'`, `'LeakyReLU'`, `'PReLU'`, `'ELU'`, `'GELU'`, `'Snake'`.\n",
    "\n",
    "* `output_activation`: the activation function of the output layer. Recommended choices are `'Sigmoid'`, `'Softmax'`, `None` (linear), `'Snake'`.\n",
    "\n",
    "* `batch_norm`: if specified as True, all convolutional layers will be configured as stacks of \"Conv2D-BN-Activation\".\n",
    "\n",
    "* `stack_num_down`: number of convolutional layers per downsampling level.\n",
    "\n",
    "* `stack_num_up`: number of convolutional layers (after concatenation) per upsampling level. \n",
    "\n",
    "* `pool`: the configuration of downsampling (encoding) blocks.\n",
    "    * `pool=False`: downsampling with a convolutional layer (2-by-2 convolution kernels with 2 strides; optional batch normalization and activation). \n",
    "    * `pool=True` or `pool='max'` downsampling with a max-pooling layer.\n",
    "    * `pool='ave'` downsampling with a average-pooling layer.\n",
    "\n",
    "\n",
    "* `unpool`: the configuration of upsampling (decoding) blocks.\n",
    "    * `unpool=False`: upsampling with a transpose convolutional layer (2-by-2 convolution kernels with 2 strides; optional batch normalization and activation). \n",
    "    * `unpool=True` or `unpool='bilinear'` upsampling with bilinear interpolation.\n",
    "    * `unpool='nearest'` upsampling with reflective padding.\n",
    "\n",
    "\n",
    "* `name`: user-specified prefix of the configured layer and model. Use `keras.models.Model.summary` to identify the exact name of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Configuring your model\n",
    "\n",
    "**Note**\n",
    "\n",
    "Configured models can be saved through `model.save(filepath, save_traces=True)`, but they may contain python objects that are not part of the `tensorflow.keras`. Thus when loading the model, it is preferred to load the weights only, and set/freeze them within a new configuration.\n",
    "\n",
    "```python\n",
    "e.g.\n",
    "\n",
    "weights = dummy_loader(model_old_path)\n",
    "model_new = swin_transformer_model(...)\n",
    "model_new.set_weights(weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: U-net for binary classification with:\n",
    "\n",
    "1. Five down- and upsampliung levels (or four downsampling levels and one bottom level).\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. One convolutional layer (after concatenation) per upsamling level.\n",
    "\n",
    "2. Gaussian Error Linear Unit (GELU) activcation, Softmax output activation, batch normalization.\n",
    "\n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512, 1024],\n",
    "    n_labels=2,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"GELU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=\"nearest\",\n",
    "    name=\"unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2**: Vnet (originally proposed for 3-d inputs, here modified for 2-d inputs) for binary classification with:\n",
    "\n",
    "1. Input size of (256, 256, 1); PReLU does not support input tensor with shapes of NoneType \n",
    "\n",
    "\n",
    "\n",
    "1. Five down- and upsampliung levels (or four downsampling levels and one bottom level).\n",
    "\n",
    "1. Number of stacked convolutional layers of the residual path increase with downsampling levels from one to three (symmetrically, decrease with upsampling levels).   \n",
    "    * `res_num_ini=1`\n",
    "    * `res_num_max=3`\n",
    "    \n",
    " \n",
    "2. PReLU activcation, Softmax output activation, batch normalization.\n",
    "\n",
    "3. Downsampling through stride convolutional layers.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PReLU\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PReLU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/layer_utils.py:235\u001b[0m, in \u001b[0;36mCONV_stack\u001b[0;34m(X, channel, kernel_size, stack_num, dilation_rate, activation, batch_norm, name)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# activation\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_activation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m(X)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/keras/src/layers/activations/activation.py:28\u001b[0m, in \u001b[0;36mActivation.__init__\u001b[0;34m(self, activation, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_masking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/keras-unet-collection/lib/python3.12/site-packages/keras/src/activations/__init__.py:104\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret activation function identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret activation function identifier: prelu",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvnet_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mres_num_ini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_num_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPReLU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvnet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_vnet_2d.py:223\u001b[0m, in \u001b[0;36mvnet_2d\u001b[0;34m(input_size, filter_num, n_labels, res_num_ini, res_num_max, activation, output_activation, batch_norm, pool, unpool, name)\u001b[0m\n\u001b[1;32m    221\u001b[0m X \u001b[38;5;241m=\u001b[39m IN\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# base\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvnet_2d_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_num_ini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_num_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_num_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mres_num_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# output layer\u001b[39;00m\n\u001b[1;32m    226\u001b[0m OUT \u001b[38;5;241m=\u001b[39m CONV_output(X, n_labels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39moutput_activation, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_output\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/_model_vnet_2d.py:143\u001b[0m, in \u001b[0;36mvnet_2d_base\u001b[0;34m(input_tensor, filter_num, res_num_ini, res_num_max, activation, batch_norm, pool, unpool, name)\u001b[0m\n\u001b[1;32m    141\u001b[0m X \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# ini conv layer\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mCONV_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_num\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m               \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_input_conv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m X \u001b[38;5;241m=\u001b[39m Res_CONV_stack(X, X, filter_num[\u001b[38;5;241m0\u001b[39m], res_num\u001b[38;5;241m=\u001b[39mres_num_list[\u001b[38;5;241m0\u001b[39m], activation\u001b[38;5;241m=\u001b[39mactivation, \n\u001b[1;32m    147\u001b[0m              batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_down_0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m    148\u001b[0m X_skip\u001b[38;5;241m.\u001b[39mappend(X)\n",
      "File \u001b[0;32m~/Source/keras-unet-collection/examples/../keras_unet_collection/layer_utils.py:238\u001b[0m, in \u001b[0;36mCONV_stack\u001b[0;34m(X, channel, kernel_size, stack_num, dilation_rate, activation, batch_norm, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mprint\u001b[39m(activation)\n\u001b[0;32m--> 238\u001b[0m         activation_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m         X \u001b[38;5;241m=\u001b[39m activation_func(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_activation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, i))(X)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PReLU' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.vnet_2d(\n",
    "    (256, 256, 1),\n",
    "    filter_num=[16, 32, 64, 128, 256],\n",
    "    n_labels=2,\n",
    "    res_num_ini=1,\n",
    "    res_num_max=3,\n",
    "    activation=\"PReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    name=\"vnet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention-Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**: attention-Unet for single target regression with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. ReLU activation, linear output activation (None), batch normalization.\n",
    "\n",
    "3. Additive attention, ReLU attention activation.\n",
    "        \n",
    "4. Downsampling through stride convolutional layers.\n",
    "\n",
    "5. Upsampling through bilinear interpolation.   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.att_unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=1,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    activation=\"ReLU\",\n",
    "    atten_activation=\"ReLU\",\n",
    "    attention=\"add\",\n",
    "    output_activation=None,\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"bilinear\",\n",
    "    name=\"attunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4**: U-net++ for three-label classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. LeakyReLU activation, Softmax output activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers.\n",
    "\n",
    "5. Deep supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_plus_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=3,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    activation=\"LeakyReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=False,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"xnet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET 3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5**: UNet 3+ for binary classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two convolutional layers per downsampling level.\n",
    "\n",
    "3. One convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "2. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through transpose convolutional layers.\n",
    "\n",
    "5. Deep supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_3plus_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_skip=\"auto\",\n",
    "    filter_num_aggregate=\"auto\",\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"unet3plus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `filter_num_skip` and `filter_num_aggregate` can be specified explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_3plus_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_skip=[64, 64, 64],\n",
    "    filter_num_aggregate=256,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"unet3plus\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 6**: R2U-net for binary classification with:\n",
    "\n",
    "1. Four down- and upsampling levels.\n",
    "\n",
    "2. Two recurrent convolutional layers with two iterations per down- and upsampling level.\n",
    "\n",
    "2. ReLU activation, Softmax output activation, no batch normalization.\n",
    "        \n",
    "3. Downsampling through Maxpooling.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.r2_unet_2d(\n",
    "    (None, None, 3),\n",
    "    [64, 128, 256, 512],\n",
    "    n_labels=2,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=1,\n",
    "    recur_num=2,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=\"max\",\n",
    "    unpool=\"nearest\",\n",
    "    name=\"r2unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResUnet-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 7**: ResUnet-a for 16-label classification with:\n",
    "\n",
    "1. input size of (128, 128, 3)\n",
    "\n",
    "1. Six downsampling levels followed by an Atrous Spatial Pyramid Pooling (ASPP) layer with 256 filters.\n",
    "\n",
    "1. Six upsampling levels followed by an ASPP layer with 128 filters.\n",
    "\n",
    "2. dilation rates of {1, 3, 15, 31} for shallow layers, {1,3,15} for intermediate layers, and {1,} for deep layers.\n",
    "\n",
    "3. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "\n",
    "4. Downsampling through stride convolutional layers.\n",
    "\n",
    "4. Upsampling through reflective padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resunet_a_2d(\n",
    "    (128, 128, 3),\n",
    "    [32, 64, 128, 256, 512, 1024],\n",
    "    dilation_num=[1, 3, 15, 31],\n",
    "    n_labels=16,\n",
    "    aspp_num_down=256,\n",
    "    aspp_num_up=128,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"nearest\",\n",
    "    name=\"resunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dilation_num` can be specified per down- and uplampling level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resunet_a_2d(\n",
    "    (128, 128, 3),\n",
    "    [32, 64, 128, 256, 512, 1024],\n",
    "    dilation_num=[\n",
    "        [1, 3, 15, 31],\n",
    "        [1, 3, 15, 31],\n",
    "        [1, 3, 15],\n",
    "        [1, 3, 15],\n",
    "        [\n",
    "            1,\n",
    "        ],\n",
    "        [\n",
    "            1,\n",
    "        ],\n",
    "    ],\n",
    "    n_labels=16,\n",
    "    aspp_num_down=256,\n",
    "    aspp_num_up=128,\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=\"nearest\",\n",
    "    name=\"resunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U^2-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 8**: U^2-Net for binary classification with:\n",
    "\n",
    "1. Six downsampling levels with the first four layers built with RSU, and the last two (one downsampling layer, one bottom layer) built with RSU-F4.\n",
    "    * `filter_num_down=[64, 128, 256, 512]`\n",
    "    * `filter_mid_num_down=[32, 32, 64, 128]`\n",
    "    * `filter_4f_num=[512, 512]`\n",
    "    * `filter_4f_mid_num=[256, 256]`\n",
    "    \n",
    "    \n",
    "1. Six upsampling levels with the deepest layer built with RSU-F4, and the other four layers built with RSU.\n",
    "    * `filter_num_up=[64, 64, 128, 256]`\n",
    "    * `filter_mid_num_up=[16, 32, 64, 128]`\n",
    "    \n",
    "    \n",
    "3. ReLU activation, Sigmoid output activation, batch normalization.\n",
    "\n",
    "4. Deep supervision\n",
    "\n",
    "5. Downsampling through stride convolutional layers.\n",
    "\n",
    "6. Upsampling through transpose convolutional layers.\n",
    "\n",
    "*In the original work of U^2-Net, down- and upsampling were achieved through maxpooling (`pool=True` or `pool='max'`) and bilinear interpolation (`unpool=True` or unpool=`'bilinear'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.u2net_2d(\n",
    "    (128, 128, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    filter_num_up=[64, 64, 128, 256],\n",
    "    filter_mid_num_down=[32, 32, 64, 128],\n",
    "    filter_mid_num_up=[16, 32, 64, 128],\n",
    "    filter_4f_num=[512, 512],\n",
    "    filter_4f_mid_num=[256, 256],\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=None,\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"u2net\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `u2net_2d` supports automated determination of filter numbers per down- and upsampling level. Auto-mode may produce a slightly larger network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.u2net_2d(\n",
    "    (None, None, 3),\n",
    "    n_labels=2,\n",
    "    filter_num_down=[64, 128, 256, 512],\n",
    "    activation=\"ReLU\",\n",
    "    output_activation=\"Sigmoid\",\n",
    "    batch_norm=True,\n",
    "    pool=False,\n",
    "    unpool=False,\n",
    "    deep_supervision=True,\n",
    "    name=\"u2net\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransUNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 9**: TransUNET for 12-label classification with:\n",
    "\n",
    "* input size of (512, 512, 3)\n",
    "\n",
    "* Four down- and upsampling levels.\n",
    "\n",
    "* Two convolutional layers per downsampling level.\n",
    "\n",
    "* Two convolutional layers (after concatenation) per upsampling level.\n",
    "\n",
    "* 12 transformer blocks (`num_transformer=12`).\n",
    "\n",
    "* 12 attention heads (`num_heads=12`).\n",
    "\n",
    "* 3072 MLP nodes per vision transformer (`num_mlp=3072`).\n",
    "\n",
    "* 768 embeding dimensions (`embed_dim=768`).\n",
    "\n",
    "* Gaussian Error Linear Unit (GELU) activcation for transformer MLPs.\n",
    "\n",
    "* ReLU activation, softmax output activation, batch normalization.\n",
    "\n",
    "* Downsampling through maxpooling.\n",
    "\n",
    "* Upsampling through bilinear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.transunet_2d(\n",
    "    (512, 512, 3),\n",
    "    filter_num=[64, 128, 256, 512],\n",
    "    n_labels=12,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    embed_dim=768,\n",
    "    num_mlp=3072,\n",
    "    num_heads=12,\n",
    "    num_transformer=12,\n",
    "    activation=\"ReLU\",\n",
    "    mlp_activation=\"GELU\",\n",
    "    output_activation=\"Softmax\",\n",
    "    batch_norm=True,\n",
    "    pool=True,\n",
    "    unpool=\"bilinear\",\n",
    "    name=\"transunet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin-UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 10**: Swin-UNET for 3-label classification with:\n",
    "\n",
    "* input size of (128, 128, 3)\n",
    "\n",
    "* Four down- and upsampling levels (or three downsampling levels and one bottom level) (`depth=4`).\n",
    "\n",
    "* Two Swin-Transformers per downsampling level.\n",
    "\n",
    "* Two Swin-Transformers (after concatenation) per upsampling level.\n",
    "\n",
    "* Extract 2-by-2 patches from the input (`patch_size=(2, 2)`)\n",
    "\n",
    "* Embed 2-by-2 patches to 64 dimensions (`filter_num_begin=64`, a.k.a, number of embedded dimensions).\n",
    "\n",
    "* Number of attention heads for each down- and upsampling level: `num_heads=[4, 8, 8, 8]`.\n",
    "\n",
    "* Size of attention windows for each down- and upsampling level: `window_size=[4, 2, 2, 2]`.\n",
    "\n",
    "* 512 nodes per Swin-Transformer (`num_mlp=512`)\n",
    "\n",
    "* Shift attention windows (i.e., Swin-MSA) (`shift_window=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.swin_unet_2d(\n",
    "    (128, 128, 3),\n",
    "    filter_num_begin=64,\n",
    "    n_labels=3,\n",
    "    depth=4,\n",
    "    stack_num_down=2,\n",
    "    stack_num_up=2,\n",
    "    patch_size=(2, 2),\n",
    "    num_heads=[4, 8, 8, 8],\n",
    "    window_size=[4, 2, 2, 2],\n",
    "    num_mlp=512,\n",
    "    output_activation=\"Softmax\",\n",
    "    shift_window=True,\n",
    "    name=\"swin_unet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-unet-collection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
